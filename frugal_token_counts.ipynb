{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e79d92a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17388b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens for gsm8k_test_80: 57497\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eccf2a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9ca5ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\n",
    "    \"argilla_news_test_50.json\",\n",
    "    \"argilla_news_test_60.json\",\n",
    "    \"argilla_news_test_80.json\",\n",
    "    # \"argilla_news_test.json\",\n",
    "    \"cosmosqa_test_50.json\",\n",
    "    \"cosmosqa_test_60.json\",\n",
    "    \"cosmosqa_test_80.json\",\n",
    "    # \"cosmosqa_test.json\",\n",
    "    \"gsm8k_test_50.json\",\n",
    "    \"gsm8k_test_60.json\",\n",
    "    \"gsm8k_test_80.json\",\n",
    "    # \"gsm8k_test.json\",\n",
    "    \"imdb_test_50.json\",\n",
    "    \"imdb_test_60.json\",\n",
    "    \"imdb_test_80.json\",\n",
    "    # \"imdb_test.json\"\n",
    "]\n",
    "attr_methods = [\"decompx\", \"globenc\"]\n",
    "model_names = ['gemini2.0_flash_thinking', 'gpt3.5', 'llama3_8b', 'llama3_70b', 'o3_mini']\n",
    "def get_path(attr_method, file_name):\n",
    "    return f'/home/cse/Rifat/FrugalFewShot/Frugal ICL/Frugal-ICL/scores/{attr_method}/{file_name}'\n",
    "def get_path2(model_name, attr_method, file_name):\n",
    "    return f'/home/cse/Rifat/FrugalFewShot/Frugal ICL/Frugal-ICL/responses/{attr_method}/{model_name}_{file_name}'\n",
    "def count_total_tokens(json_path):\n",
    "    file = os.path.basename(json_path)\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "    total_tokens = 0\n",
    "    text = \"\"\n",
    "    for i, item in enumerate(data):\n",
    "        if \"frugal_text\" not in item:\n",
    "            text = item.get(\"text\") or item.get(\"question\") or item.get(\"context\") or \"bruh\"\n",
    "        else:\n",
    "            text = item.get(\"frugal_text\", \"\")\n",
    "        token_count = len(encoding.encode(text))\n",
    "        total_tokens += token_count\n",
    "        # print(f\"Item {i+1}: {token_count} tokens\")\n",
    "    print(f\"Total tokens for {file}: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9bde5299",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "with open('api/config.json') as f:\n",
    "    config = json.load(f)\n",
    "login(config['HUGGING_FACE_TOKEN']) # put your User Access Tokens here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c973ceff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
    "def count_total_tokens_llama3_8b(json_path):\n",
    "    file = os.path.basename(json_path)\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    total_tokens = 0\n",
    "    text = \"\"\n",
    "    for i, item in enumerate(data):\n",
    "        if \"frugal_text\" not in item:\n",
    "            text = item.get(\"text\") or item.get(\"question\") or item.get(\"context\") or \"bruh\"\n",
    "        else:\n",
    "            text = item.get(\"frugal_text\", \"\")\n",
    "        tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "        token_count = len(tokens)\n",
    "        total_tokens += token_count\n",
    "        # print(f\"Item {i+1}: {token_count} tokens\")\n",
    "    print(f\"Total tokens for {file}: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "15ef905d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2 = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-70B\")\n",
    "def count_total_tokens_llama3_70b(json_path):\n",
    "    file = os.path.basename(json_path)\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    total_tokens = 0\n",
    "    text = \"\"\n",
    "    for i, item in enumerate(data):\n",
    "        if \"frugal_text\" not in item:\n",
    "            text = item.get(\"text\") or item.get(\"question\") or item.get(\"context\") or \"bruh\"\n",
    "        else:\n",
    "            text = item.get(\"frugal_text\", \"\")\n",
    "        tokens = tokenizer2.encode(text, add_special_tokens=False)\n",
    "        token_count = len(tokens)\n",
    "        total_tokens += token_count\n",
    "        # print(f\"Item {i+1}: {token_count} tokens\")\n",
    "    print(f\"Total tokens for {file}: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3a048881",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer3 = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "def count_total_tokens_gemini2_flash_thinking(json_path):\n",
    "    file = os.path.basename(json_path)\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    total_tokens = 0\n",
    "    text = \"\"\n",
    "    for i, item in enumerate(data):\n",
    "        if \"frugal_text\" not in item:\n",
    "            text = item.get(\"text\") or item.get(\"question\") or item.get(\"context\") or \"bruh\"\n",
    "        else:\n",
    "            text = item.get(\"frugal_text\", \"\")\n",
    "        tokens = tokenizer3.encode(text, add_special_tokens=False)\n",
    "        token_count = len(tokens)\n",
    "        total_tokens += token_count\n",
    "        # print(f\"Item {i+1}: {token_count} tokens\")\n",
    "    print(f\"Total tokens for {file}: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1431b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens for argilla_news_test: 459597\n",
      "Total tokens for cosmosqa_test: 10657\n",
      "Total tokens for gsm8k_test: 59250\n",
      "Total tokens for imdb_test: 288452\n"
     ]
    }
   ],
   "source": [
    "# for attr_method in attr_methods:\n",
    "for file_name in filenames:\n",
    "    json_path = get_path(attr_method, file_name)\n",
    "    count_total_tokens(json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bcf15cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens for o3_mini_argilla_news_test_50.json: 255031\n",
      "Total tokens for o3_mini_argilla_news_test_60.json: 306132\n",
      "Total tokens for o3_mini_argilla_news_test_80.json: 408742\n",
      "Total tokens for o3_mini_cosmosqa_test_50.json: 41869\n",
      "Total tokens for o3_mini_cosmosqa_test_60.json: 49583\n",
      "Total tokens for o3_mini_cosmosqa_test_80.json: 65040\n",
      "Total tokens for o3_mini_gsm8k_test_50.json: 37963\n",
      "Total tokens for o3_mini_gsm8k_test_60.json: 44299\n",
      "Total tokens for o3_mini_gsm8k_test_80.json: 57497\n",
      "Total tokens for o3_mini_imdb_test_50.json: 154897\n",
      "Total tokens for o3_mini_imdb_test_60.json: 185780\n",
      "Total tokens for o3_mini_imdb_test_80.json: 247243\n",
      "Total tokens for o3_mini_argilla_news_test_50.json: 262712\n",
      "Total tokens for o3_mini_argilla_news_test_60.json: 308109\n",
      "Total tokens for o3_mini_argilla_news_test_80.json: 396808\n",
      "Total tokens for o3_mini_cosmosqa_test_50.json: 39471\n",
      "Total tokens for o3_mini_cosmosqa_test_60.json: 46634\n",
      "Total tokens for o3_mini_cosmosqa_test_80.json: 60916\n",
      "Total tokens for o3_mini_gsm8k_test_50.json: 34860\n",
      "Total tokens for o3_mini_gsm8k_test_60.json: 40601\n",
      "Total tokens for o3_mini_gsm8k_test_80.json: 51899\n",
      "Total tokens for o3_mini_imdb_test_50.json: 163110\n",
      "Total tokens for o3_mini_imdb_test_60.json: 192253\n",
      "Total tokens for o3_mini_imdb_test_80.json: 249937\n"
     ]
    }
   ],
   "source": [
    "for attr_method in attr_methods:\n",
    "    for file_name in filenames:\n",
    "        json_path = get_path2('o3_mini', attr_method, file_name)\n",
    "        try:\n",
    "            count_total_tokens(json_path)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab50f8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens for llama3_8b_argilla_news_test_50.json: 251017\n",
      "Total tokens for llama3_8b_argilla_news_test_60.json: 301222\n",
      "Total tokens for llama3_8b_argilla_news_test_80.json: 401888\n",
      "Total tokens for llama3_8b_cosmosqa_test_50.json: 41857\n",
      "Total tokens for llama3_8b_cosmosqa_test_60.json: 49571\n",
      "Total tokens for llama3_8b_cosmosqa_test_80.json: 65026\n",
      "Total tokens for llama3_8b_gsm8k_test_50.json: 37947\n",
      "Total tokens for llama3_8b_gsm8k_test_60.json: 44282\n",
      "Total tokens for llama3_8b_gsm8k_test_80.json: 57480\n",
      "Total tokens for llama3_8b_imdb_test_50.json: 154830\n",
      "Total tokens for llama3_8b_imdb_test_60.json: 185690\n",
      "Total tokens for llama3_8b_imdb_test_80.json: 249854\n",
      "Total tokens for llama3_8b_argilla_news_test_50.json: 262359\n",
      "Total tokens for llama3_8b_argilla_news_test_60.json: 307752\n",
      "Total tokens for llama3_8b_argilla_news_test_80.json: 396444\n",
      "Total tokens for llama3_8b_cosmosqa_test_50.json: 39447\n",
      "Total tokens for llama3_8b_cosmosqa_test_60.json: 46610\n",
      "Total tokens for llama3_8b_cosmosqa_test_80.json: 60892\n",
      "Total tokens for llama3_8b_gsm8k_test_50.json: 34836\n",
      "Total tokens for llama3_8b_gsm8k_test_60.json: 40577\n",
      "Total tokens for llama3_8b_gsm8k_test_80.json: 51872\n",
      "Total tokens for llama3_8b_imdb_test_50.json: 163028\n",
      "Total tokens for llama3_8b_imdb_test_60.json: 192170\n",
      "Total tokens for llama3_8b_imdb_test_80.json: 249854\n"
     ]
    }
   ],
   "source": [
    "for attr_method in attr_methods:\n",
    "    for file_name in filenames:\n",
    "        json_path = get_path2('llama3_8b', attr_method, file_name)\n",
    "        try:\n",
    "            count_total_tokens_llama3_8b(json_path)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "14b040b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens for llama3_70b_argilla_news_test_50.json: 251017\n",
      "Total tokens for llama3_70b_argilla_news_test_60.json: 301222\n",
      "Total tokens for llama3_70b_argilla_news_test_80.json: 401888\n",
      "Total tokens for llama3_70b_cosmosqa_test_50.json: 41857\n",
      "Total tokens for llama3_70b_cosmosqa_test_60.json: 49571\n",
      "Total tokens for llama3_70b_cosmosqa_test_80.json: 65026\n",
      "Total tokens for llama3_70b_gsm8k_test_50.json: 37947\n",
      "Total tokens for llama3_70b_gsm8k_test_60.json: 44282\n",
      "Total tokens for llama3_70b_gsm8k_test_80.json: 57480\n",
      "Total tokens for llama3_70b_imdb_test_50.json: 154830\n",
      "Total tokens for llama3_70b_imdb_test_60.json: 185690\n",
      "Total tokens for llama3_70b_imdb_test_80.json: 249854\n",
      "Total tokens for llama3_70b_argilla_news_test_50.json: 262359\n",
      "Total tokens for llama3_70b_argilla_news_test_60.json: 307752\n",
      "Total tokens for llama3_70b_argilla_news_test_80.json: 396444\n",
      "Total tokens for llama3_70b_cosmosqa_test_50.json: 39447\n",
      "Total tokens for llama3_70b_cosmosqa_test_60.json: 46610\n",
      "Total tokens for llama3_70b_cosmosqa_test_80.json: 60892\n",
      "Total tokens for llama3_70b_gsm8k_test_50.json: 34836\n",
      "Total tokens for llama3_70b_gsm8k_test_60.json: 40577\n",
      "Total tokens for llama3_70b_gsm8k_test_80.json: 51872\n",
      "Total tokens for llama3_70b_imdb_test_50.json: 163028\n",
      "Total tokens for llama3_70b_imdb_test_60.json: 192170\n",
      "Total tokens for llama3_70b_imdb_test_80.json: 249854\n"
     ]
    }
   ],
   "source": [
    "for attr_method in attr_methods:\n",
    "    for file_name in filenames:\n",
    "        json_path = get_path2('llama3_70b', attr_method, file_name)\n",
    "        try:\n",
    "            count_total_tokens_llama3_8b(json_path)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "74ad0194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: /home/cse/Rifat/FrugalFewShot/Frugal ICL/Frugal-ICL/responses/decompx/gemini2.0_flash_thinking_argilla_news_test_50.json\n",
      "File not found: /home/cse/Rifat/FrugalFewShot/Frugal ICL/Frugal-ICL/responses/decompx/gemini2.0_flash_thinking_argilla_news_test_60.json\n",
      "File not found: /home/cse/Rifat/FrugalFewShot/Frugal ICL/Frugal-ICL/responses/decompx/gemini2.0_flash_thinking_argilla_news_test_80.json\n",
      "Total tokens for gemini2.0_flash_thinking_cosmosqa_test_50.json: 41857\n",
      "Total tokens for gemini2.0_flash_thinking_cosmosqa_test_60.json: 49571\n",
      "Total tokens for gemini2.0_flash_thinking_cosmosqa_test_80.json: 65026\n",
      "File not found: /home/cse/Rifat/FrugalFewShot/Frugal ICL/Frugal-ICL/responses/decompx/gemini2.0_flash_thinking_gsm8k_test_50.json\n",
      "File not found: /home/cse/Rifat/FrugalFewShot/Frugal ICL/Frugal-ICL/responses/decompx/gemini2.0_flash_thinking_gsm8k_test_60.json\n",
      "Total tokens for gemini2.0_flash_thinking_gsm8k_test_80.json: 57480\n",
      "File not found: /home/cse/Rifat/FrugalFewShot/Frugal ICL/Frugal-ICL/responses/decompx/gemini2.0_flash_thinking_imdb_test_50.json\n",
      "File not found: /home/cse/Rifat/FrugalFewShot/Frugal ICL/Frugal-ICL/responses/decompx/gemini2.0_flash_thinking_imdb_test_60.json\n",
      "File not found: /home/cse/Rifat/FrugalFewShot/Frugal ICL/Frugal-ICL/responses/decompx/gemini2.0_flash_thinking_imdb_test_80.json\n",
      "File not found: /home/cse/Rifat/FrugalFewShot/Frugal ICL/Frugal-ICL/responses/globenc/gemini2.0_flash_thinking_argilla_news_test_50.json\n",
      "File not found: /home/cse/Rifat/FrugalFewShot/Frugal ICL/Frugal-ICL/responses/globenc/gemini2.0_flash_thinking_argilla_news_test_60.json\n",
      "File not found: /home/cse/Rifat/FrugalFewShot/Frugal ICL/Frugal-ICL/responses/globenc/gemini2.0_flash_thinking_argilla_news_test_80.json\n",
      "Total tokens for gemini2.0_flash_thinking_cosmosqa_test_50.json: 39447\n",
      "Total tokens for gemini2.0_flash_thinking_cosmosqa_test_60.json: 46610\n",
      "Total tokens for gemini2.0_flash_thinking_cosmosqa_test_80.json: 60892\n",
      "Total tokens for gemini2.0_flash_thinking_gsm8k_test_50.json: 34836\n",
      "Total tokens for gemini2.0_flash_thinking_gsm8k_test_60.json: 40577\n",
      "Total tokens for gemini2.0_flash_thinking_gsm8k_test_80.json: 51872\n",
      "File not found: /home/cse/Rifat/FrugalFewShot/Frugal ICL/Frugal-ICL/responses/globenc/gemini2.0_flash_thinking_imdb_test_50.json\n",
      "File not found: /home/cse/Rifat/FrugalFewShot/Frugal ICL/Frugal-ICL/responses/globenc/gemini2.0_flash_thinking_imdb_test_60.json\n",
      "File not found: /home/cse/Rifat/FrugalFewShot/Frugal ICL/Frugal-ICL/responses/globenc/gemini2.0_flash_thinking_imdb_test_80.json\n"
     ]
    }
   ],
   "source": [
    "for attr_method in attr_methods:\n",
    "    for file_name in filenames:\n",
    "        json_path = get_path2('gemini2.0_flash_thinking', attr_method, file_name)\n",
    "        try:\n",
    "            count_total_tokens_llama3_8b(json_path)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316537f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rifat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
